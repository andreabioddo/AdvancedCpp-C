{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rPFlmzQnEloZ"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabioddo/AdvancedCpp-C/blob/main/Distributed_Learning_Project5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install torch and torchvi"
      ],
      "metadata": {
        "id": "rPFlmzQnEloZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaveP5eCDkCO",
        "outputId": "fa0c815d-77ef-4c37-8150-cfe05f636039",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install dataset CIFAR-100"
      ],
      "metadata": {
        "id": "LqSl_5NUFTDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# # split dataset for workers\n",
        "# def split_dataset(dataset, num_workers):\n",
        "#     \"\"\"将数据集拆分为 num_workers 份\"\"\"\n",
        "#     worker_datasets = torch.utils.data.random_split(\n",
        "#         dataset, [len(dataset) // num_workers] * num_workers\n",
        "#     )\n",
        "#     return worker_datasets\n",
        "\n",
        "# # Number of workers\n",
        "# K = 4  # Assume 4 workers\n",
        "# Bloc = 64  # Local batch size\n",
        "# worker_datasets = split_dataset(train_dataset, K)\n",
        "# worker_loaders = [\n",
        "#     torch.utils.data.DataLoader(ds, batch_size=Bloc, shuffle=True) for ds in worker_datasets\n",
        "# ]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oP1XDeqExVw",
        "outputId": "779f1f03-35e0-4f02-f744-43a7005cd081"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 30.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centralized baseline"
      ],
      "metadata": {
        "id": "KpPjPiOTFatY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define LeNet-5 model\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 100)\n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training function\n",
        "def train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_accuracies.append(100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Test function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
        "\n",
        "# Training configurations\n",
        "num_epochs = 150\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train with SGDM\n",
        "model = LeNet5().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "print(\"Training with SGDM optimizer...\")\n",
        "train_losses_sgdm, val_losses_sgdm, train_acc_sgdm, val_acc_sgdm = train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs)\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# Train with AdamW\n",
        "model = LeNet5().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "print(\"Training with AdamW optimizer...\")\n",
        "train_losses_adamw, val_losses_adamw, train_acc_adamw, val_acc_adamw = train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs)\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# Plot training and validation results\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, title):\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{title} Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, label='Val Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title(f'{title} Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot results for SGDM\n",
        "plot_results(train_losses_sgdm, val_losses_sgdm, train_acc_sgdm, val_acc_sgdm, 'SGDM')\n",
        "\n",
        "# Plot results for AdamW\n",
        "plot_results(train_losses_adamw, val_losses_adamw, train_acc_adamw, val_acc_adamw, 'AdamW')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6FFqW7ZFkWQ",
        "outputId": "19e2bdc6-8bf6-4c8a-e8c7-3c97c01d16d9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGDM optimizer...\n",
            "Epoch 1/150, Train Loss: 4.6046, Train Acc: 1.02%, Val Loss: 4.5926, Val Acc: 1.06%\n",
            "Epoch 2/150, Train Loss: 4.3955, Train Acc: 3.42%, Val Loss: 4.2224, Val Acc: 5.64%\n",
            "Epoch 3/150, Train Loss: 4.0067, Train Acc: 8.14%, Val Loss: 3.8442, Val Acc: 9.69%\n",
            "Epoch 4/150, Train Loss: 3.7833, Train Acc: 11.10%, Val Loss: 3.6930, Val Acc: 12.23%\n",
            "Epoch 5/150, Train Loss: 3.6510, Train Acc: 13.29%, Val Loss: 3.5684, Val Acc: 14.68%\n",
            "Epoch 6/150, Train Loss: 3.5346, Train Acc: 15.21%, Val Loss: 3.5284, Val Acc: 15.45%\n",
            "Epoch 7/150, Train Loss: 3.4570, Train Acc: 16.82%, Val Loss: 3.4481, Val Acc: 17.42%\n",
            "Epoch 8/150, Train Loss: 3.3758, Train Acc: 18.26%, Val Loss: 3.4220, Val Acc: 17.33%\n",
            "Epoch 9/150, Train Loss: 3.3119, Train Acc: 19.39%, Val Loss: 3.3478, Val Acc: 19.82%\n",
            "Epoch 10/150, Train Loss: 3.2610, Train Acc: 20.30%, Val Loss: 3.2423, Val Acc: 21.42%\n",
            "Epoch 11/150, Train Loss: 3.2087, Train Acc: 21.50%, Val Loss: 3.2949, Val Acc: 20.18%\n",
            "Epoch 12/150, Train Loss: 3.1633, Train Acc: 22.36%, Val Loss: 3.1870, Val Acc: 22.35%\n",
            "Epoch 13/150, Train Loss: 3.1157, Train Acc: 22.88%, Val Loss: 3.2433, Val Acc: 21.56%\n",
            "Epoch 14/150, Train Loss: 3.0768, Train Acc: 23.84%, Val Loss: 3.1074, Val Acc: 24.29%\n",
            "Epoch 15/150, Train Loss: 3.0204, Train Acc: 25.16%, Val Loss: 3.1206, Val Acc: 23.32%\n",
            "Epoch 16/150, Train Loss: 2.9845, Train Acc: 25.71%, Val Loss: 3.0624, Val Acc: 24.92%\n",
            "Epoch 17/150, Train Loss: 2.9528, Train Acc: 26.45%, Val Loss: 3.0537, Val Acc: 25.78%\n",
            "Epoch 18/150, Train Loss: 2.9156, Train Acc: 27.17%, Val Loss: 3.0089, Val Acc: 25.85%\n",
            "Epoch 19/150, Train Loss: 2.8970, Train Acc: 27.84%, Val Loss: 2.9895, Val Acc: 27.14%\n",
            "Epoch 20/150, Train Loss: 2.8677, Train Acc: 28.12%, Val Loss: 2.9797, Val Acc: 26.86%\n",
            "Epoch 21/150, Train Loss: 2.8575, Train Acc: 28.39%, Val Loss: 2.9791, Val Acc: 27.05%\n",
            "Epoch 22/150, Train Loss: 2.8312, Train Acc: 29.11%, Val Loss: 3.0601, Val Acc: 24.83%\n",
            "Epoch 23/150, Train Loss: 2.8110, Train Acc: 29.54%, Val Loss: 2.9534, Val Acc: 27.37%\n",
            "Epoch 24/150, Train Loss: 2.7865, Train Acc: 29.68%, Val Loss: 2.9858, Val Acc: 25.95%\n",
            "Epoch 25/150, Train Loss: 2.7748, Train Acc: 29.96%, Val Loss: 3.0268, Val Acc: 26.37%\n",
            "Epoch 26/150, Train Loss: 2.7566, Train Acc: 30.16%, Val Loss: 2.9213, Val Acc: 27.85%\n",
            "Epoch 27/150, Train Loss: 2.7467, Train Acc: 30.55%, Val Loss: 2.8865, Val Acc: 28.61%\n",
            "Epoch 28/150, Train Loss: 2.7253, Train Acc: 31.19%, Val Loss: 2.9211, Val Acc: 27.84%\n",
            "Epoch 29/150, Train Loss: 2.7090, Train Acc: 31.35%, Val Loss: 2.8864, Val Acc: 28.44%\n",
            "Epoch 30/150, Train Loss: 2.7025, Train Acc: 31.79%, Val Loss: 2.8913, Val Acc: 28.29%\n",
            "Epoch 31/150, Train Loss: 2.6833, Train Acc: 32.02%, Val Loss: 2.8969, Val Acc: 28.50%\n",
            "Epoch 32/150, Train Loss: 2.6727, Train Acc: 32.18%, Val Loss: 2.8872, Val Acc: 28.98%\n",
            "Epoch 33/150, Train Loss: 2.6602, Train Acc: 32.55%, Val Loss: 2.8424, Val Acc: 29.12%\n",
            "Epoch 34/150, Train Loss: 2.6506, Train Acc: 32.61%, Val Loss: 2.8184, Val Acc: 29.89%\n",
            "Epoch 35/150, Train Loss: 2.6363, Train Acc: 32.87%, Val Loss: 2.8617, Val Acc: 29.73%\n",
            "Epoch 36/150, Train Loss: 2.6255, Train Acc: 32.91%, Val Loss: 2.8931, Val Acc: 29.23%\n",
            "Epoch 37/150, Train Loss: 2.6144, Train Acc: 33.39%, Val Loss: 2.8558, Val Acc: 29.09%\n",
            "Epoch 38/150, Train Loss: 2.5977, Train Acc: 33.68%, Val Loss: 2.8210, Val Acc: 29.94%\n",
            "Epoch 39/150, Train Loss: 2.6008, Train Acc: 33.68%, Val Loss: 2.8086, Val Acc: 29.64%\n",
            "Epoch 40/150, Train Loss: 2.5850, Train Acc: 33.93%, Val Loss: 2.8724, Val Acc: 29.29%\n",
            "Epoch 41/150, Train Loss: 2.5662, Train Acc: 34.38%, Val Loss: 2.8863, Val Acc: 28.77%\n",
            "Epoch 42/150, Train Loss: 2.5665, Train Acc: 34.27%, Val Loss: 2.8744, Val Acc: 29.10%\n",
            "Epoch 43/150, Train Loss: 2.5630, Train Acc: 34.34%, Val Loss: 2.7933, Val Acc: 30.55%\n",
            "Epoch 44/150, Train Loss: 2.5527, Train Acc: 34.41%, Val Loss: 2.7652, Val Acc: 31.36%\n",
            "Epoch 45/150, Train Loss: 2.5437, Train Acc: 34.85%, Val Loss: 2.8390, Val Acc: 29.85%\n",
            "Epoch 46/150, Train Loss: 2.5329, Train Acc: 34.86%, Val Loss: 2.7889, Val Acc: 29.61%\n",
            "Epoch 47/150, Train Loss: 2.5243, Train Acc: 35.14%, Val Loss: 2.9012, Val Acc: 28.71%\n",
            "Epoch 48/150, Train Loss: 2.5066, Train Acc: 35.69%, Val Loss: 2.8494, Val Acc: 29.47%\n",
            "Epoch 49/150, Train Loss: 2.5010, Train Acc: 35.70%, Val Loss: 2.7879, Val Acc: 31.17%\n",
            "Epoch 50/150, Train Loss: 2.4846, Train Acc: 36.12%, Val Loss: 2.8452, Val Acc: 30.18%\n",
            "Epoch 51/150, Train Loss: 2.4874, Train Acc: 36.01%, Val Loss: 2.8204, Val Acc: 29.50%\n",
            "Epoch 52/150, Train Loss: 2.4648, Train Acc: 36.58%, Val Loss: 2.7554, Val Acc: 31.76%\n",
            "Epoch 53/150, Train Loss: 2.4614, Train Acc: 36.48%, Val Loss: 2.7422, Val Acc: 31.30%\n",
            "Epoch 54/150, Train Loss: 2.4513, Train Acc: 36.50%, Val Loss: 2.7464, Val Acc: 31.53%\n",
            "Epoch 55/150, Train Loss: 2.4361, Train Acc: 37.02%, Val Loss: 2.7462, Val Acc: 31.31%\n",
            "Epoch 56/150, Train Loss: 2.4376, Train Acc: 37.00%, Val Loss: 2.7785, Val Acc: 30.81%\n",
            "Epoch 57/150, Train Loss: 2.4252, Train Acc: 37.38%, Val Loss: 2.8463, Val Acc: 30.43%\n",
            "Epoch 58/150, Train Loss: 2.4116, Train Acc: 37.40%, Val Loss: 2.7603, Val Acc: 31.03%\n",
            "Epoch 59/150, Train Loss: 2.4075, Train Acc: 37.64%, Val Loss: 2.8209, Val Acc: 30.62%\n",
            "Epoch 60/150, Train Loss: 2.3978, Train Acc: 37.91%, Val Loss: 2.7841, Val Acc: 30.87%\n",
            "Epoch 61/150, Train Loss: 2.3935, Train Acc: 38.19%, Val Loss: 2.7568, Val Acc: 31.96%\n",
            "Epoch 62/150, Train Loss: 2.3700, Train Acc: 38.65%, Val Loss: 2.7866, Val Acc: 30.80%\n",
            "Epoch 63/150, Train Loss: 2.3770, Train Acc: 38.34%, Val Loss: 2.7574, Val Acc: 31.82%\n",
            "Epoch 64/150, Train Loss: 2.3571, Train Acc: 38.68%, Val Loss: 2.7365, Val Acc: 31.34%\n",
            "Epoch 65/150, Train Loss: 2.3438, Train Acc: 39.05%, Val Loss: 2.7348, Val Acc: 31.88%\n",
            "Epoch 66/150, Train Loss: 2.3408, Train Acc: 39.28%, Val Loss: 2.7365, Val Acc: 31.84%\n",
            "Epoch 67/150, Train Loss: 2.3302, Train Acc: 39.19%, Val Loss: 2.7616, Val Acc: 31.82%\n",
            "Epoch 68/150, Train Loss: 2.3128, Train Acc: 39.57%, Val Loss: 2.7355, Val Acc: 31.99%\n",
            "Epoch 69/150, Train Loss: 2.3029, Train Acc: 40.00%, Val Loss: 2.6913, Val Acc: 33.00%\n",
            "Epoch 70/150, Train Loss: 2.2942, Train Acc: 40.08%, Val Loss: 2.7298, Val Acc: 32.55%\n",
            "Epoch 71/150, Train Loss: 2.2819, Train Acc: 40.46%, Val Loss: 2.6900, Val Acc: 33.21%\n",
            "Epoch 72/150, Train Loss: 2.2734, Train Acc: 40.65%, Val Loss: 2.7539, Val Acc: 31.86%\n",
            "Epoch 73/150, Train Loss: 2.2621, Train Acc: 40.81%, Val Loss: 2.6971, Val Acc: 32.71%\n",
            "Epoch 74/150, Train Loss: 2.2527, Train Acc: 41.03%, Val Loss: 2.7107, Val Acc: 32.85%\n",
            "Epoch 75/150, Train Loss: 2.2465, Train Acc: 40.87%, Val Loss: 2.7252, Val Acc: 32.86%\n",
            "Epoch 76/150, Train Loss: 2.2200, Train Acc: 41.84%, Val Loss: 2.7070, Val Acc: 32.63%\n",
            "Epoch 77/150, Train Loss: 2.2194, Train Acc: 41.87%, Val Loss: 2.7161, Val Acc: 32.20%\n",
            "Epoch 78/150, Train Loss: 2.2017, Train Acc: 41.92%, Val Loss: 2.7090, Val Acc: 32.42%\n",
            "Epoch 79/150, Train Loss: 2.1894, Train Acc: 42.35%, Val Loss: 2.7310, Val Acc: 33.14%\n",
            "Epoch 80/150, Train Loss: 2.1825, Train Acc: 42.76%, Val Loss: 2.7206, Val Acc: 33.01%\n",
            "Epoch 81/150, Train Loss: 2.1725, Train Acc: 42.59%, Val Loss: 2.6877, Val Acc: 33.53%\n",
            "Epoch 82/150, Train Loss: 2.1517, Train Acc: 43.41%, Val Loss: 2.7080, Val Acc: 33.03%\n",
            "Epoch 83/150, Train Loss: 2.1471, Train Acc: 43.34%, Val Loss: 2.7104, Val Acc: 32.50%\n",
            "Epoch 84/150, Train Loss: 2.1304, Train Acc: 43.45%, Val Loss: 2.6780, Val Acc: 33.51%\n",
            "Epoch 85/150, Train Loss: 2.1212, Train Acc: 43.77%, Val Loss: 2.7001, Val Acc: 33.76%\n",
            "Epoch 86/150, Train Loss: 2.1036, Train Acc: 44.02%, Val Loss: 2.6977, Val Acc: 33.46%\n",
            "Epoch 87/150, Train Loss: 2.0923, Train Acc: 44.57%, Val Loss: 2.7001, Val Acc: 33.02%\n",
            "Epoch 88/150, Train Loss: 2.0849, Train Acc: 44.64%, Val Loss: 2.8144, Val Acc: 31.19%\n",
            "Epoch 89/150, Train Loss: 2.0626, Train Acc: 44.91%, Val Loss: 2.7319, Val Acc: 33.19%\n",
            "Epoch 90/150, Train Loss: 2.0536, Train Acc: 45.47%, Val Loss: 2.7236, Val Acc: 33.24%\n",
            "Epoch 91/150, Train Loss: 2.0423, Train Acc: 45.58%, Val Loss: 2.7088, Val Acc: 32.89%\n",
            "Epoch 92/150, Train Loss: 2.0318, Train Acc: 45.53%, Val Loss: 2.7206, Val Acc: 33.14%\n",
            "Epoch 93/150, Train Loss: 2.0129, Train Acc: 46.22%, Val Loss: 2.7013, Val Acc: 33.73%\n",
            "Epoch 94/150, Train Loss: 2.0044, Train Acc: 46.44%, Val Loss: 2.6657, Val Acc: 34.16%\n",
            "Epoch 95/150, Train Loss: 1.9873, Train Acc: 47.02%, Val Loss: 2.7056, Val Acc: 33.96%\n",
            "Epoch 96/150, Train Loss: 1.9736, Train Acc: 47.47%, Val Loss: 2.7161, Val Acc: 33.58%\n",
            "Epoch 97/150, Train Loss: 1.9645, Train Acc: 47.53%, Val Loss: 2.6991, Val Acc: 33.75%\n",
            "Epoch 98/150, Train Loss: 1.9433, Train Acc: 47.94%, Val Loss: 2.6907, Val Acc: 33.81%\n",
            "Epoch 99/150, Train Loss: 1.9322, Train Acc: 48.27%, Val Loss: 2.6926, Val Acc: 34.07%\n",
            "Epoch 100/150, Train Loss: 1.9155, Train Acc: 48.74%, Val Loss: 2.7184, Val Acc: 33.81%\n",
            "Epoch 101/150, Train Loss: 1.9021, Train Acc: 48.93%, Val Loss: 2.7251, Val Acc: 33.67%\n",
            "Epoch 102/150, Train Loss: 1.8893, Train Acc: 49.29%, Val Loss: 2.6871, Val Acc: 34.15%\n",
            "Epoch 103/150, Train Loss: 1.8702, Train Acc: 49.62%, Val Loss: 2.6807, Val Acc: 34.33%\n",
            "Epoch 104/150, Train Loss: 1.8553, Train Acc: 50.27%, Val Loss: 2.7141, Val Acc: 34.05%\n",
            "Epoch 105/150, Train Loss: 1.8466, Train Acc: 50.35%, Val Loss: 2.6817, Val Acc: 34.46%\n",
            "Epoch 106/150, Train Loss: 1.8298, Train Acc: 50.97%, Val Loss: 2.6945, Val Acc: 34.32%\n",
            "Epoch 107/150, Train Loss: 1.8138, Train Acc: 50.93%, Val Loss: 2.7289, Val Acc: 34.04%\n",
            "Epoch 108/150, Train Loss: 1.7983, Train Acc: 51.44%, Val Loss: 2.7273, Val Acc: 34.04%\n",
            "Epoch 109/150, Train Loss: 1.7764, Train Acc: 51.97%, Val Loss: 2.7192, Val Acc: 34.33%\n",
            "Epoch 110/150, Train Loss: 1.7668, Train Acc: 52.30%, Val Loss: 2.7289, Val Acc: 34.18%\n",
            "Epoch 111/150, Train Loss: 1.7465, Train Acc: 52.84%, Val Loss: 2.7109, Val Acc: 34.25%\n",
            "Epoch 112/150, Train Loss: 1.7341, Train Acc: 53.26%, Val Loss: 2.7120, Val Acc: 34.63%\n",
            "Epoch 113/150, Train Loss: 1.7169, Train Acc: 53.68%, Val Loss: 2.6983, Val Acc: 34.80%\n",
            "Epoch 114/150, Train Loss: 1.7048, Train Acc: 54.10%, Val Loss: 2.7451, Val Acc: 33.74%\n",
            "Epoch 115/150, Train Loss: 1.6892, Train Acc: 54.38%, Val Loss: 2.7112, Val Acc: 34.64%\n",
            "Epoch 116/150, Train Loss: 1.6771, Train Acc: 54.45%, Val Loss: 2.7202, Val Acc: 34.19%\n",
            "Epoch 117/150, Train Loss: 1.6580, Train Acc: 54.98%, Val Loss: 2.7543, Val Acc: 34.77%\n",
            "Epoch 118/150, Train Loss: 1.6400, Train Acc: 55.67%, Val Loss: 2.7369, Val Acc: 35.02%\n",
            "Epoch 119/150, Train Loss: 1.6258, Train Acc: 56.03%, Val Loss: 2.7253, Val Acc: 34.94%\n",
            "Epoch 120/150, Train Loss: 1.6127, Train Acc: 56.20%, Val Loss: 2.7428, Val Acc: 34.70%\n",
            "Epoch 121/150, Train Loss: 1.5980, Train Acc: 56.85%, Val Loss: 2.7363, Val Acc: 34.87%\n",
            "Epoch 122/150, Train Loss: 1.5856, Train Acc: 57.31%, Val Loss: 2.7490, Val Acc: 34.76%\n",
            "Epoch 123/150, Train Loss: 1.5680, Train Acc: 57.55%, Val Loss: 2.7434, Val Acc: 35.02%\n",
            "Epoch 124/150, Train Loss: 1.5565, Train Acc: 57.99%, Val Loss: 2.7475, Val Acc: 34.68%\n",
            "Epoch 125/150, Train Loss: 1.5397, Train Acc: 58.42%, Val Loss: 2.7501, Val Acc: 34.94%\n",
            "Epoch 126/150, Train Loss: 1.5298, Train Acc: 58.95%, Val Loss: 2.7557, Val Acc: 34.84%\n",
            "Epoch 127/150, Train Loss: 1.5138, Train Acc: 59.23%, Val Loss: 2.7478, Val Acc: 34.86%\n",
            "Epoch 128/150, Train Loss: 1.5022, Train Acc: 59.47%, Val Loss: 2.7517, Val Acc: 35.14%\n",
            "Epoch 129/150, Train Loss: 1.4879, Train Acc: 60.00%, Val Loss: 2.7544, Val Acc: 35.43%\n",
            "Epoch 130/150, Train Loss: 1.4769, Train Acc: 60.23%, Val Loss: 2.7624, Val Acc: 35.11%\n",
            "Epoch 131/150, Train Loss: 1.4657, Train Acc: 60.77%, Val Loss: 2.7597, Val Acc: 35.57%\n",
            "Epoch 132/150, Train Loss: 1.4532, Train Acc: 61.04%, Val Loss: 2.7619, Val Acc: 35.30%\n",
            "Epoch 133/150, Train Loss: 1.4408, Train Acc: 61.41%, Val Loss: 2.7664, Val Acc: 35.38%\n",
            "Epoch 134/150, Train Loss: 1.4308, Train Acc: 61.88%, Val Loss: 2.7753, Val Acc: 35.25%\n",
            "Epoch 135/150, Train Loss: 1.4203, Train Acc: 62.13%, Val Loss: 2.7809, Val Acc: 35.03%\n",
            "Epoch 136/150, Train Loss: 1.4116, Train Acc: 62.34%, Val Loss: 2.7723, Val Acc: 35.38%\n",
            "Epoch 137/150, Train Loss: 1.4023, Train Acc: 62.60%, Val Loss: 2.7793, Val Acc: 35.24%\n",
            "Epoch 138/150, Train Loss: 1.3934, Train Acc: 63.06%, Val Loss: 2.7826, Val Acc: 35.38%\n",
            "Epoch 139/150, Train Loss: 1.3860, Train Acc: 63.31%, Val Loss: 2.7891, Val Acc: 35.49%\n",
            "Epoch 140/150, Train Loss: 1.3794, Train Acc: 63.40%, Val Loss: 2.7898, Val Acc: 35.25%\n",
            "Epoch 141/150, Train Loss: 1.3731, Train Acc: 63.68%, Val Loss: 2.7845, Val Acc: 35.60%\n",
            "Epoch 142/150, Train Loss: 1.3668, Train Acc: 63.91%, Val Loss: 2.7896, Val Acc: 35.50%\n",
            "Epoch 143/150, Train Loss: 1.3617, Train Acc: 64.08%, Val Loss: 2.7917, Val Acc: 35.51%\n",
            "Epoch 144/150, Train Loss: 1.3571, Train Acc: 64.27%, Val Loss: 2.7896, Val Acc: 35.58%\n",
            "Epoch 145/150, Train Loss: 1.3534, Train Acc: 64.41%, Val Loss: 2.7879, Val Acc: 35.45%\n",
            "Epoch 146/150, Train Loss: 1.3500, Train Acc: 64.56%, Val Loss: 2.7892, Val Acc: 35.59%\n",
            "Epoch 147/150, Train Loss: 1.3472, Train Acc: 64.59%, Val Loss: 2.7897, Val Acc: 35.52%\n",
            "Epoch 148/150, Train Loss: 1.3451, Train Acc: 64.65%, Val Loss: 2.7890, Val Acc: 35.51%\n",
            "Epoch 149/150, Train Loss: 1.3435, Train Acc: 64.80%, Val Loss: 2.7887, Val Acc: 35.52%\n",
            "Epoch 150/150, Train Loss: 1.3425, Train Acc: 64.84%, Val Loss: 2.7888, Val Acc: 35.51%\n",
            "Test Accuracy: 35.12%\n",
            "Training with AdamW optimizer...\n",
            "Epoch 1/150, Train Loss: 4.0425, Train Acc: 7.96%, Val Loss: 3.7595, Val Acc: 12.06%\n",
            "Epoch 2/150, Train Loss: 3.5960, Train Acc: 15.06%, Val Loss: 3.4965, Val Acc: 16.49%\n",
            "Epoch 3/150, Train Loss: 3.3643, Train Acc: 18.92%, Val Loss: 3.2997, Val Acc: 20.27%\n",
            "Epoch 4/150, Train Loss: 3.2014, Train Acc: 21.85%, Val Loss: 3.2332, Val Acc: 22.11%\n",
            "Epoch 5/150, Train Loss: 3.0802, Train Acc: 23.95%, Val Loss: 3.1481, Val Acc: 23.59%\n",
            "Epoch 6/150, Train Loss: 2.9879, Train Acc: 25.64%, Val Loss: 3.0654, Val Acc: 24.96%\n",
            "Epoch 7/150, Train Loss: 2.9101, Train Acc: 27.50%, Val Loss: 3.0413, Val Acc: 25.68%\n",
            "Epoch 8/150, Train Loss: 2.8496, Train Acc: 28.65%, Val Loss: 3.0335, Val Acc: 26.16%\n",
            "Epoch 9/150, Train Loss: 2.7872, Train Acc: 30.00%, Val Loss: 2.9976, Val Acc: 26.63%\n",
            "Epoch 10/150, Train Loss: 2.7358, Train Acc: 30.88%, Val Loss: 3.0053, Val Acc: 26.61%\n",
            "Epoch 11/150, Train Loss: 2.6935, Train Acc: 31.79%, Val Loss: 2.9885, Val Acc: 27.19%\n",
            "Epoch 12/150, Train Loss: 2.6498, Train Acc: 32.58%, Val Loss: 2.9631, Val Acc: 27.66%\n",
            "Epoch 13/150, Train Loss: 2.6076, Train Acc: 33.66%, Val Loss: 2.9531, Val Acc: 28.31%\n",
            "Epoch 14/150, Train Loss: 2.5742, Train Acc: 34.15%, Val Loss: 2.9378, Val Acc: 28.91%\n",
            "Epoch 15/150, Train Loss: 2.5459, Train Acc: 34.62%, Val Loss: 2.9484, Val Acc: 28.72%\n",
            "Epoch 16/150, Train Loss: 2.5105, Train Acc: 35.45%, Val Loss: 2.9525, Val Acc: 28.38%\n",
            "Epoch 17/150, Train Loss: 2.4818, Train Acc: 35.84%, Val Loss: 2.9573, Val Acc: 28.44%\n",
            "Epoch 18/150, Train Loss: 2.4618, Train Acc: 36.39%, Val Loss: 2.9753, Val Acc: 28.76%\n",
            "Epoch 19/150, Train Loss: 2.4302, Train Acc: 37.09%, Val Loss: 2.9645, Val Acc: 29.20%\n",
            "Epoch 20/150, Train Loss: 2.4059, Train Acc: 37.66%, Val Loss: 2.9675, Val Acc: 29.36%\n",
            "Epoch 21/150, Train Loss: 2.3887, Train Acc: 37.88%, Val Loss: 2.9398, Val Acc: 29.32%\n",
            "Epoch 22/150, Train Loss: 2.3635, Train Acc: 38.41%, Val Loss: 2.9734, Val Acc: 28.61%\n",
            "Epoch 23/150, Train Loss: 2.3415, Train Acc: 38.94%, Val Loss: 2.9787, Val Acc: 29.16%\n",
            "Epoch 24/150, Train Loss: 2.3204, Train Acc: 39.42%, Val Loss: 2.9821, Val Acc: 28.99%\n",
            "Epoch 25/150, Train Loss: 2.3033, Train Acc: 39.95%, Val Loss: 2.9975, Val Acc: 28.69%\n",
            "Epoch 26/150, Train Loss: 2.2807, Train Acc: 40.40%, Val Loss: 2.9797, Val Acc: 29.23%\n",
            "Epoch 27/150, Train Loss: 2.2645, Train Acc: 40.74%, Val Loss: 3.0238, Val Acc: 29.40%\n",
            "Epoch 28/150, Train Loss: 2.2495, Train Acc: 40.72%, Val Loss: 3.0382, Val Acc: 28.94%\n",
            "Epoch 29/150, Train Loss: 2.2268, Train Acc: 41.27%, Val Loss: 3.0472, Val Acc: 29.22%\n",
            "Epoch 30/150, Train Loss: 2.2145, Train Acc: 41.74%, Val Loss: 3.0357, Val Acc: 29.36%\n",
            "Epoch 31/150, Train Loss: 2.2020, Train Acc: 42.17%, Val Loss: 3.0739, Val Acc: 28.07%\n",
            "Epoch 32/150, Train Loss: 2.1834, Train Acc: 42.38%, Val Loss: 3.0365, Val Acc: 29.23%\n",
            "Epoch 33/150, Train Loss: 2.1709, Train Acc: 42.53%, Val Loss: 3.0489, Val Acc: 28.49%\n",
            "Epoch 34/150, Train Loss: 2.1499, Train Acc: 43.19%, Val Loss: 3.0848, Val Acc: 29.05%\n",
            "Epoch 35/150, Train Loss: 2.1361, Train Acc: 43.75%, Val Loss: 3.1133, Val Acc: 28.54%\n",
            "Epoch 36/150, Train Loss: 2.1209, Train Acc: 44.00%, Val Loss: 3.0832, Val Acc: 29.32%\n",
            "Epoch 37/150, Train Loss: 2.1118, Train Acc: 43.96%, Val Loss: 3.1408, Val Acc: 28.06%\n",
            "Epoch 38/150, Train Loss: 2.0994, Train Acc: 44.20%, Val Loss: 3.1235, Val Acc: 28.27%\n",
            "Epoch 39/150, Train Loss: 2.0807, Train Acc: 44.64%, Val Loss: 3.1344, Val Acc: 28.47%\n",
            "Epoch 40/150, Train Loss: 2.0677, Train Acc: 44.98%, Val Loss: 3.1459, Val Acc: 29.04%\n",
            "Epoch 41/150, Train Loss: 2.0591, Train Acc: 45.18%, Val Loss: 3.2012, Val Acc: 28.31%\n",
            "Epoch 42/150, Train Loss: 2.0433, Train Acc: 45.38%, Val Loss: 3.1761, Val Acc: 28.69%\n",
            "Epoch 43/150, Train Loss: 2.0293, Train Acc: 45.85%, Val Loss: 3.1780, Val Acc: 28.53%\n",
            "Epoch 44/150, Train Loss: 2.0171, Train Acc: 46.33%, Val Loss: 3.1617, Val Acc: 28.44%\n",
            "Epoch 45/150, Train Loss: 2.0047, Train Acc: 46.55%, Val Loss: 3.2116, Val Acc: 28.46%\n",
            "Epoch 46/150, Train Loss: 1.9965, Train Acc: 46.66%, Val Loss: 3.2089, Val Acc: 28.85%\n",
            "Epoch 47/150, Train Loss: 1.9868, Train Acc: 46.66%, Val Loss: 3.2386, Val Acc: 28.27%\n",
            "Epoch 48/150, Train Loss: 1.9709, Train Acc: 47.00%, Val Loss: 3.2474, Val Acc: 28.13%\n",
            "Epoch 49/150, Train Loss: 1.9590, Train Acc: 47.59%, Val Loss: 3.2408, Val Acc: 28.50%\n",
            "Epoch 50/150, Train Loss: 1.9516, Train Acc: 47.52%, Val Loss: 3.2590, Val Acc: 28.20%\n",
            "Epoch 51/150, Train Loss: 1.9401, Train Acc: 47.90%, Val Loss: 3.2463, Val Acc: 27.90%\n",
            "Epoch 52/150, Train Loss: 1.9258, Train Acc: 48.29%, Val Loss: 3.3041, Val Acc: 28.04%\n",
            "Epoch 53/150, Train Loss: 1.9130, Train Acc: 48.61%, Val Loss: 3.3428, Val Acc: 27.86%\n",
            "Epoch 54/150, Train Loss: 1.9034, Train Acc: 48.66%, Val Loss: 3.3019, Val Acc: 28.15%\n",
            "Epoch 55/150, Train Loss: 1.8971, Train Acc: 48.77%, Val Loss: 3.3661, Val Acc: 27.85%\n",
            "Epoch 56/150, Train Loss: 1.8825, Train Acc: 49.05%, Val Loss: 3.3285, Val Acc: 28.25%\n",
            "Epoch 57/150, Train Loss: 1.8716, Train Acc: 49.20%, Val Loss: 3.3388, Val Acc: 27.97%\n",
            "Epoch 58/150, Train Loss: 1.8638, Train Acc: 49.71%, Val Loss: 3.3702, Val Acc: 28.04%\n",
            "Epoch 59/150, Train Loss: 1.8581, Train Acc: 49.65%, Val Loss: 3.3829, Val Acc: 28.16%\n",
            "Epoch 60/150, Train Loss: 1.8449, Train Acc: 50.01%, Val Loss: 3.3718, Val Acc: 27.85%\n",
            "Epoch 61/150, Train Loss: 1.8351, Train Acc: 50.41%, Val Loss: 3.4424, Val Acc: 27.52%\n",
            "Epoch 62/150, Train Loss: 1.8297, Train Acc: 50.29%, Val Loss: 3.4100, Val Acc: 27.92%\n",
            "Epoch 63/150, Train Loss: 1.8178, Train Acc: 50.57%, Val Loss: 3.4384, Val Acc: 27.92%\n",
            "Epoch 64/150, Train Loss: 1.8049, Train Acc: 51.23%, Val Loss: 3.4626, Val Acc: 26.91%\n",
            "Epoch 65/150, Train Loss: 1.7994, Train Acc: 51.06%, Val Loss: 3.4735, Val Acc: 27.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPPYaRako9Zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}