{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n"
      ],
      "metadata": {
        "id": "KlHheqAGVUVY",
        "outputId": "26d9e5cb-7872-40f7-f785-27d5b33effd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "u2T4ia3AsnT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "GPmLs0QzJQrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yoNOiteP6yy",
        "outputId": "d3b2b5d1-3bed-44e5-c43b-5aff328e3426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Costants"
      ],
      "metadata": {
        "id": "9vT0d5W_srbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "Vtu0U8wLQISx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "bNvH-6hEsuxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "MhXxXjgks2K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_hyperparams(train_loader, val_loader, test_loader, hyperparams, num_epochs, device, checkpoint_path, type_of_optimizer, batch_size):\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    if type_of_optimizer == \"SGDM\":\n",
        "        if hyperparams.get('momentum', None) is None:\n",
        "            raise ValueError(\"Momentum is required for SGDM\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for SGDM\")\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for SGDM\")\n",
        "\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            momentum=hyperparams['momentum'],\n",
        "            weight_decay=hyperparams['weight_decay']\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"AdamW\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for AdamW\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for AdamW\")\n",
        "\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            eps=hyperparams['eps']\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"LAMB\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for LAMB\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for LAMB\")\n",
        "\n",
        "        optimizer = LAMB(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            #betas=hyperparams.get('betas', (0.9, 0.999)),\n",
        "            #eps=hyperparams.get('eps', 1e-6)\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"LARS\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for LARS\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for LARS\")\n",
        "\n",
        "        optimizer = LARS(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            #momentum=hyperparams.get('momentum', 0.9),\n",
        "            #eta=hyperparams.get('eta', 0.001),\n",
        "            #eps=hyperparams.get('eps', 1e-8)\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer type\")\n",
        "\n",
        "    iterations_per_epoch = len(train_dataset) // batch_size\n",
        "\n",
        "    # warmup iterations for 5 epochs\n",
        "    wu_it = 5 * iterations_per_epoch\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=wu_it)\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, CosineAnnealingLR(optimizer, T_max=num_epochs - wu_it)], milestones=[5])\n",
        "\n",
        "\n",
        "    import time\n",
        "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    os.makedirs(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", exist_ok=True)\n",
        "\n",
        "    metrics_files = {\n",
        "        'train_acc': os.path.join(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", f'train_accuracy_{timestamp}_{str(batch_size)}.txt'),\n",
        "        'val_acc': os.path.join(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", f'val_accuracy_{timestamp}_{str(batch_size)}.txt'),\n",
        "        'train_loss': os.path.join(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", f'train_loss_{timestamp}_{str(batch_size)}.txt'),\n",
        "        'val_loss': os.path.join(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", f'val_loss_{timestamp}_{str(batch_size)}.txt')\n",
        "    }\n",
        "\n",
        "    # Crea un file di configurazione per salvare i parametri dell'esperimento\n",
        "    with open(os.path.join(\"/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison\", f'experiment_config_{timestamp}_{str(batch_size)}.txt'), 'w') as f:\n",
        "        f.write(f\"Experiment Configuration:\\n\")\n",
        "        f.write(f\"Optimizer: {type_of_optimizer}\\n\")\n",
        "        f.write(f\"Hyperparameters: {str(hyperparams)}\\n\")\n",
        "        f.write(f\"Number of epochs: {num_epochs}\\n\")\n",
        "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    start_epoch = 0\n",
        "\n",
        "\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss_total, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            train_loss_total += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss_total, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss_total += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss_total / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        try:\n",
        "            with open(metrics_files['train_acc'], 'a') as f:\n",
        "                f.write(f\"{epoch + 1},{train_acc:.4f}\\n\")\n",
        "            with open(metrics_files['val_acc'], 'a') as f:\n",
        "                f.write(f\"{epoch + 1},{val_acc:.4f}\\n\")\n",
        "            with open(metrics_files['train_loss'], 'a') as f:\n",
        "                f.write(f\"{epoch + 1},{train_loss:.4f}\\n\")\n",
        "            with open(metrics_files['val_loss'], 'a') as f:\n",
        "                f.write(f\"{epoch + 1},{val_loss:.4f}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(device_type=device.type):\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc"
      ],
      "metadata": {
        "id": "mc0vfd9tNvNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining different optimizers"
      ],
      "metadata": {
        "id": "IgKVfV0Ts7GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaAvJoRuJNPF"
      },
      "outputs": [],
      "source": [
        "class LARS(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, beta1=0.9, batch_size=32, weight_decay=1e-4, epsilon=1e-8):\n",
        "        defaults = dict(lr=lr, beta1=beta1, batch_size=batch_size,\n",
        "                       weight_decay=weight_decay, epsilon=epsilon)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "        # Initialize momentum\n",
        "        self.m0 = {param: torch.zeros_like(param.data) for group in self.param_groups\n",
        "                   for param in group['params']}\n",
        "\n",
        "    def phi(self, norm):\n",
        "        \"\"\"Scaling function φ as defined in the paper\"\"\"\n",
        "        return torch.ones_like(norm)  # Can be modified for different scaling strategies\n",
        "\n",
        "    def get_batch_samples(self, dataset, batch_size):\n",
        "        \"\"\"Draw b samples from the dataset\"\"\"\n",
        "        indices = torch.randperm(len(dataset))[:batch_size]\n",
        "        return [dataset[i] for i in indices]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None, dataset=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1 = group['beta1']\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "            epsilon = group['epsilon']\n",
        "\n",
        "            # Draw batch samples if dataset provided\n",
        "            if dataset is not None:\n",
        "                batch = self.get_batch_samples(dataset, group['batch_size'])\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad\n",
        "                state = self.state[param]\n",
        "\n",
        "                # Compute gt (gradient plus weight decay)\n",
        "                gt = grad + weight_decay * param.data\n",
        "\n",
        "                # Update momentum mt\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(param.data)\n",
        "                mt = state['momentum']\n",
        "                mt.mul_(beta1).add_((1 - beta1) * (gt + weight_decay * param.data))\n",
        "\n",
        "                # Compute the update with φ scaling\n",
        "                param_norm = param.data.norm(2).clamp(min=epsilon)\n",
        "                mt_norm = mt.norm(2).clamp(min=epsilon)\n",
        "\n",
        "                update = lr * (self.phi(param_norm) / mt_norm) * mt\n",
        "\n",
        "                # Update parameters\n",
        "                param.data.sub_(update)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, batch_size=32):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                       weight_decay=weight_decay,\n",
        "                       batch_size=batch_size)\n",
        "        super(LAMB, self).__init__(params, defaults)\n",
        "\n",
        "    def phi(self, norm):\n",
        "        \"\"\"Scaling function φ as defined in the paper\"\"\"\n",
        "        return torch.ones_like(norm)  # Default implementation\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Momentum term m\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    # Velocity term v\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                eps = group['eps']\n",
        "                lr = group['lr']\n",
        "                weight_decay = group['weight_decay']\n",
        "\n",
        "                state['step'] += 1\n",
        "                t = state['step']\n",
        "\n",
        "                # Update biased first and second moment estimates\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)  # v_t\n",
        "\n",
        "                # Bias correction\n",
        "                bias_correction1 = 1 - beta1 ** t\n",
        "                bias_correction2 = 1 - beta2 ** t\n",
        "\n",
        "                m_t = exp_avg / bias_correction1\n",
        "                v_t = exp_avg_sq / bias_correction2\n",
        "\n",
        "                # Compute ratio r_t\n",
        "                r_t = m_t / (torch.sqrt(v_t) + eps)\n",
        "\n",
        "                # Add weight decay\n",
        "                if weight_decay != 0:\n",
        "                    r_t.add_(p.data, alpha=weight_decay)\n",
        "\n",
        "                # Compute trust ratio\n",
        "                param_norm = p.data.norm(2)\n",
        "                update_norm = r_t.norm(2)\n",
        "\n",
        "                # Compute the local learning rate\n",
        "                if param_norm != 0 and update_norm != 0:\n",
        "                    local_lr = self.phi(param_norm) / update_norm\n",
        "                else:\n",
        "                    local_lr = 1.0\n",
        "\n",
        "                # Final update\n",
        "                p.data.add_(r_t, alpha=-lr * local_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Large-Batch Optimizer Selection')\n",
        "    parser.add_argument('--optimizer', type=str, default='SGDM', choices=['SGDM', 'AdamW', 'LARS', 'LAMB'],\n",
        "                        help='Select optimizer (SGDM, AdamW, LARS, LAMB)')\n",
        "    #parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n",
        "    #parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    args = parser.parse_args()\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "iybuJXWdLzIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get train, test and val dataset"
      ],
      "metadata": {
        "id": "k8oCJ42etBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(batch_size=100):\n",
        "\n",
        "    print(\"batch_size\", batch_size)\n",
        "    # Define the transform to only convert the images to tensors (without normalization yet)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    print(\"Mean: \", mean)\n",
        "    print(\"Std: \", std)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "    print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Debugging: Check DataLoader outputs\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "        if i == 10:  # Test first 10 batches\n",
        "            break\n",
        "    print(f\"Data loading for 10 batches completed.\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "UEqcoElZtH_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    #args = parse_args()\n",
        "\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.optimizer = 'SGDM'  # Choose from: 'SGDM', 'AdamW', 'LARS', 'LAMB'\n",
        "            #self.batch_size = 32\n",
        "\n",
        "    args = Args() # replace \"args = parse_args()\" with this line\n",
        "\n",
        "\n",
        "    all_train_losses = {}\n",
        "    all_val_losses = {}\n",
        "    all_train_accuracies = {}\n",
        "    all_val_accuracies = {}\n",
        "    all_test_accuracies = {}\n",
        "\n",
        "    batch_sizes = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "\n",
        "        train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "\n",
        "        base_lr = 0.01\n",
        "        base_batch_size = 64\n",
        "        # square root scaling rule\n",
        "        lr = base_lr * math.sqrt(batch_size/base_batch_size)\n",
        "\n",
        "        hyperparams = {\n",
        "            'lr': lr,\n",
        "            'weight_decay': 0.001,\n",
        "            'momentum': 0.9,\n",
        "            'patience': 100\n",
        "        }\n",
        "\n",
        "        print(f\"Using hyperparams: lr: {hyperparams['lr']}, weight_decay: {hyperparams['weight_decay']}, momentum: {hyperparams['momentum']}\")\n",
        "\n",
        "        # Train with the best hyperparameters\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies, test_acc = train_model_with_hyperparams(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            hyperparams=hyperparams,\n",
        "            num_epochs=150,\n",
        "            device=device,\n",
        "            checkpoint_path=f'/content/drive/MyDrive/checkpoint1-{batch_size}.pth',\n",
        "            type_of_optimizer=\"LARS\",\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "        # Salvataggio dei risultati\n",
        "        all_train_losses[batch_size] = train_losses\n",
        "        all_val_losses[batch_size] = val_losses\n",
        "        all_train_accuracies[batch_size] = train_accuracies\n",
        "        all_val_accuracies[batch_size] = val_accuracies\n",
        "        all_test_accuracies[batch_size] = test_acc\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for batch_size in batch_sizes:\n",
        "        plt.plot(range(1, len(all_train_losses[batch_size]) + 1), all_train_losses[batch_size], label=f'Train Loss (BS={batch_size})')\n",
        "        plt.plot(range(1, len(all_val_losses[batch_size]) + 1), all_val_losses[batch_size], '--', label=f'Val Loss (BS={batch_size})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss for Different Batch Sizes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for batch_size in batch_sizes:\n",
        "        plt.plot(range(1, len(all_train_accuracies[batch_size]) + 1), all_train_accuracies[batch_size], label=f'Train Acc (BS={batch_size})')\n",
        "        plt.plot(range(1, len(all_val_accuracies[batch_size]) + 1), all_val_accuracies[batch_size], '--', label=f'Val Acc (BS={batch_size})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy for Different Batch Sizes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = '/content/drive/My Drive/Colab Notebooks/Traning_summary/Batch_Size_Comparison'\n",
        "    plt.savefig(save_path + '_results.png')\n",
        "    print(\"Batch size comparison results saved to Google Drive as 'Batch_Size_Comparison_results.png'\")\n",
        "    plt.show()\n",
        "\n",
        "    summary_path = save_path + '_summary.txt'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        for batch_size in batch_sizes:\n",
        "            f.write(f\"Batch Size: {batch_size}\\n\")\n",
        "            f.write(f\"Final Test Accuracy: {all_test_accuracies[batch_size]:.2f}%\\n\")\n",
        "            f.write(f\"Train Losses: {all_train_losses[batch_size]}\\n\")\n",
        "            f.write(f\"Validation Losses: {all_val_losses[batch_size]}\\n\")\n",
        "            f.write(f\"Train Accuracies: {all_train_accuracies[batch_size]}\\n\")\n",
        "            f.write(f\"Validation Accuracies: {all_val_accuracies[batch_size]}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"Training summaries saved to Google Drive as 'Batch_Size_Comparison_summary.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "7_I7B_2eJX9x",
        "outputId": "f68c7616-a09f-4955-a628-d104d808b38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size 128\n",
            "Files already downloaded and verified\n",
            "Mean:  tensor([0.5071, 0.4865, 0.4409])\n",
            "Std:  tensor([0.2667, 0.2558, 0.2754])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.82 seconds\n",
            "Batch 0: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 1: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 2: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 3: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 4: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 5: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 6: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 7: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 8: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 9: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Batch 10: inputs shape: torch.Size([128, 3, 32, 32]), labels shape: torch.Size([128])\n",
            "Data loading for 10 batches completed.\n",
            "Using hyperparams: lr: 0.01, weight_decay: 0.005, momentum: 0.9\n",
            "Epoch 1/150 | Train Loss: 4.5963, Train Acc: 1.78% | Val Loss: 4.5792, Val Acc: 2.43%\n",
            "Epoch 2/150 | Train Loss: 4.4439, Train Acc: 4.40% | Val Loss: 4.2924, Val Acc: 5.55%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-19df5f629bc5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Train with the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         train_losses, val_losses, train_accuracies, val_accuracies, test_acc = train_model_with_hyperparams(\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8e786cd8fa9a>\u001b[0m in \u001b[0;36mtrain_model_with_hyperparams\u001b[0;34m(train_loader, val_loader, test_loader, hyperparams, num_epochs, device, checkpoint_path, type_of_optimizer)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mval_loss_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttrRHudzjPEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}